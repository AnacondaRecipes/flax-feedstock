{% set name = "flax" %}
{% set version = "0.6.2" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: a6247b412f14466fefcc70d043bd0facf72552d322acdda8a8700285308d390f

build:
  skip: True # [py<36]
  number: 0
  script: {{ PYTHON }} -m pip install . -vv

requirements:
  host:
    - python
    - pip
    - setuptools
    - wheel
  run:
    - python
    - jax >=0.3.16
    - matplotlib-base
    - msgpack-python
    - numpy >=1.12
    - optax
    - rich >=11.1
    - typing_extensions >=4.1.1
    - pyyaml >=5.4.1
    - tensorstore

test:
  imports:
    - flax
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/google/flax
  summary: 'Flax: A neural network library for JAX designed for flexibility'
  description: |
    Flax is a high-performance neural network library and ecosystem for JAX that is designed for flexibility: Try new forms of training by forking an example and by modifying the training loop, not by adding features to a framework.

    Flax is being developed in close collaboration with the JAX team and comes with everything you need to start your research, including:
      - Neural network API (`flax.linen`): Dense, Conv, {Batch|Layer|Group} Norm, Attention, Pooling, {LSTM|GRU} Cell, Dropout
      - Utilities and patterns: replicated training, serialization and checkpointing, metrics, prefetching on device
      - Educational examples that work out of the box: MNIST, LSTM seq2seq, Graph Neural Networks, Sequence Tagging
      - Fast, tuned large-scale end-to-end examples: CIFAR10, ResNet on ImageNet, Transformer LM1b
  license: Apache-2.0
  license_family: Apache
  license_file: LICENSE
  doc_url: https://flax.readthedocs.io
  dev_url: https://github.com/google/flax

extra:
  recipe-maintainers:
    - PhilipVinc
    - jheek
    - avital
    - levskaya
    - andsteing
